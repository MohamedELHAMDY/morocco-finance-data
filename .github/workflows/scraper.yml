name: Scrape and Clean Data

on:
  schedule:
    - cron: "0 0 * * *"  # Runs every day at midnight UTC
  workflow_dispatch:  # Allows manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      - name: Install dependencies and set up Chrome
        run: |
          # Install dependencies for Chrome and chromedriver
          sudo apt-get update
          sudo apt-get install -y wget unzip
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo dpkg -i google-chrome-stable_current_amd64.deb
          sudo apt-get install -f
          
          # Install chromedriver
          CHROMEDRIVER_VERSION=$(wget -qO- https://chromedriver.storage.googleapis.com/LATEST_RELEASE)
          wget https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip
          unzip chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin

          # Install Python dependencies
          pip install requests pandas beautifulsoup4 selenium

      - name: Run scraper
        run: |
          export CHROME_BIN="/usr/bin/google-chrome-stable"  # Set the path for Chrome
          export PATH=$PATH:/usr/local/bin  # Ensure chromedriver is in PATH
          python scrape_data.py
